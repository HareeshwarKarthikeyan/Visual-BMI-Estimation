{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face embeddings using VGG_FACE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeoWQtcvLvamJJoegFPY6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HareeshwarKarthikeyan/Visual-BMI-Estimation/blob/main/Face_embeddings_using_VGG_FACE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y_mLcStwsmC"
      },
      "source": [
        "Importing the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzp8N2FePXHp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import face_recognition\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI1Ff46GPYkh",
        "outputId": "2ee94382-6a67-48b2-ca98-cf1cedf6c182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XOaz4Bwzlc"
      },
      "source": [
        "Importing the BMI data from the csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hRdB_N-Pdhq",
        "outputId": "ed07445d-c2a1-4b27-cd50-93ae2c694f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "image_csv=pd.read_csv(\"/gdrive/My Drive/data/annotation.csv\")\n",
        "image_csv.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of       image  height  weight        BMI\n",
              "0     f_001    1.55    61.0  25.390219\n",
              "1     f_002    1.76    85.0  27.440599\n",
              "2     f_003    1.78    56.0  17.674536\n",
              "3     f_004    1.63    63.0  23.711845\n",
              "4     f_005    1.76    54.0  17.432851\n",
              "...     ...     ...     ...        ...\n",
              "1021  m_509    1.91   116.0  31.797374\n",
              "1022  m_510    1.93   111.0  29.799458\n",
              "1023  m_511    1.88   109.0  30.839746\n",
              "1024  m_512    1.78    75.0  23.671254\n",
              "1025  m_513    2.21   137.0  28.050204\n",
              "\n",
              "[1026 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aq3vmnHw5fA"
      },
      "source": [
        "Finding out the number of images in the data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNCp6pAwPnTx",
        "outputId": "55b53a0d-a785-446f-e872-cd3e5f744a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "data_folder = \"/gdrive/My Drive/data/data\"\n",
        "from glob import glob\n",
        "all_files = glob(data_folder+\"/*\")\n",
        "all_jpgs = sorted([img for img in all_files if \".jpg\" in img or \".jpeg\" in img or \"JPG\" in img])\n",
        "print(\"Total {} photos \".format(len(all_jpgs)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 1026 photos \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTU5treKxEac"
      },
      "source": [
        "Adding the image path column to the dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qlSXUx6PwOR"
      },
      "source": [
        "from pathlib import Path as p\n",
        "image_csv['image']=all_jpgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLOAuK1odFd5"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import bz2\n",
        "filepath =\"/gdrive/My Drive/data/mmod_human_face_detector.dat.bz2\"\n",
        "zipfile = bz2.BZ2File(filepath) # open the file\n",
        "data = zipfile.read() # get the decompressed data\n",
        "newfilepath = filepath[:-4] # assuming the filepath ends with .bz2\n",
        "open(newfilepath, 'wb').write(data) # write a uncompressed file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdY6BBVxOw7"
      },
      "source": [
        "Define VGG_FACE_MODEL architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ush10mjhBfv"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\t\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "# Load VGG Face model weights\n",
        "model.load_weights('/gdrive/My Drive/data/vgg_face_weights.h5')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YkpyeO-xVbY"
      },
      "source": [
        "We need not classify whether the image consists of a face or not so we remove the final softmax layer in the output in order to find out only the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhnZy4E0gTRL"
      },
      "source": [
        "vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWNUdCq8xqMO"
      },
      "source": [
        "For every single image we load it with size(224,224) and we find the embedding vector by passing it to the neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPK1623QlkC-"
      },
      "source": [
        "def get_encoding(image_path):\n",
        "    picture=load_img(image_path,target_size=(224,224),color_mode='rgb')\n",
        "    picture=img_to_array(picture)\n",
        "    img_encode=[]\n",
        "    print(image_path)\n",
        "    picture=np.expand_dims(picture,axis=0)\n",
        "    picture=preprocess_input(picture)\n",
        "    img_encode=vgg_face(picture)\n",
        "    return img_encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXXV2Vt5mqSp"
      },
      "source": [
        "encodings=[]\n",
        "for images in image_csv.image:\n",
        "    face_enc = get_encoding(images)\n",
        "    encodings.append(np.squeeze(K.eval(face_enc)).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_5ZPvaQnb2H",
        "outputId": "394683da-bf6a-4d4b-d49c-9f388fc773fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x=np.array(encodings)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1026, 2622)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIazZxdArz-J"
      },
      "source": [
        "y_BMI = image_csv.BMI.values"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2kqKPDyEmh"
      },
      "source": [
        "x-contains embeddings for 1026 images\n",
        "\n",
        "y_BMI-contains the corresponding BMI values for the 1026 images"
      ]
    }
  ]
}